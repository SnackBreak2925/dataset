import os
import json
import dotenv
import re
import pandas as pd
from sqlalchemy import create_engine, text
from collections import defaultdict, Counter
import html
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.cluster import AgglomerativeClustering

EMAIL_REGEX = r'(([^<>()\[\]\\.,;:\s@"]+(\.[^<>()\[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))'
URL_REGEX = r"https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()!@:%_\+.~#?&\/\/=]*)"
LOGIN_REGEX = r'(–ª–æ–≥–∏–Ω|login)[^\w–∞-—è—ë]*:?\s*([\w.@!#$%^&*()_+=\-\[\]{}:;"\'<>.,?/~]+)'
PASSWORD_REGEX = (
    r'(–ø–∞—Ä–æ–ª—å|password)[^\w–∞-—è—ë]*:?\s*([\w!@#$%^&*()_+=\-\[\]{}:;"\'<>.,?/~]+)'
)
IGNORE_GROUP_PATTERN = re.compile(
    r"\((\s*–ª–æ–≥–∏–Ω\s*(?:–∏|–∏–ª–∏|,|/)?\s*–ø–∞—Ä–æ–ª—å\s*|\s*–ø–∞—Ä–æ–ª—å\s*(?:–∏|–∏–ª–∏|,|/)?\s*–ª–æ–≥–∏–Ω\s*)\)",
    flags=re.IGNORECASE,
)


def load_db_config():
    dotenv.load_dotenv()
    return {
        "host": os.getenv("DB_HOST"),
        "port": os.getenv("DB_PORT"),
        "user": os.getenv("DB_USER"),
        "password": os.getenv("DB_PASSWORD"),
        "database": os.getenv("DB_NAME"),
        "dialect": os.getenv("DB_DIALECT"),
    }


def get_engine():
    db_config = load_db_config()
    connection_string = (
        f"{db_config['dialect']}://{db_config['user']}:{db_config['password']}"
        f"@{db_config['host']}:{db_config['port']}/{db_config['database']}"
    )
    return create_engine(
        connection_string,
        connect_args={"charset": "utf8mb3", "collation": "utf8mb3_unicode_ci"},
        pool_pre_ping=True,
    )


def repiles_query():
    return r"""
SELECT
    messages.id,
    tickets.id AS ticket_id,
    tickets.subject AS subject,
    tickets.message AS ticket_message,
    messages.message AS reply_message,
    category.name AS category_title,
    messages.dt AS created_at,
    messages.staffid
FROM hesk_replies AS messages
INNER JOIN hesk_tickets AS tickets ON tickets.id = messages.replyto
INNER JOIN hesk_categories  AS category ON category.id = tickets.category
LEFT JOIN hesk_users AS staff ON messages.staffid = staff.id
WHERE
    tickets.status = 3
    AND tickets.staffreplies > 0
ORDER BY ticket_id, created_at
"""


def clean_text(text_data):
    if not isinstance(text_data, str) or not text_data.strip():
        return ""
    text = html.unescape(text_data)
    substitutions = [
        (r"<[^>]+>", " "),
        (r"<br\s*/?>", " "),
        (EMAIL_REGEX, "[EMAIL]"),
        (r"\[SIGNATURE\]", ""),
        (URL_REGEX, "[URL]"),
        (r"[\r\n]+", " "),
        (r"\s{2,}", " "),
        (r"\s+\.", "."),
        (r"(?:\[URL\]\s*){2,}", "[URL] "),
    ]
    for pat, repl in substitutions:
        text = re.sub(pat, repl, text, flags=re.IGNORECASE)

    ignore_spans = [m.span() for m in IGNORE_GROUP_PATTERN.finditer(text)]

    def in_ignore_spans(pos):
        for s, e in ignore_spans:
            if pos >= s and pos < e:
                return True
        return False

    def mask_login(match):
        start = match.start(1)
        if in_ignore_spans(start):
            return match.group(0)
        value = match.group(2).strip()
        s = match.string
        pos = match.start(1)
        left_context = s[pos - 1] if pos > 0 else ""
        right_context = s[match.end(2)] if match.end(2) < len(s) else ""
        # –ù–µ –º–∞—Å–∫–∏—Ä—É–µ–º –µ—Å–ª–∏ value ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å–æ—é–∑ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –ª–∏–±–æ value –ø—É—Å—Ç–æ–π
        if not value or value in ("–∏", "–∏–ª–∏", "/", ",", "–∏/–∏–ª–∏"):
            return match.group(0)
        if left_context in "\"'¬´" or right_context in "\"'¬ª":
            return match.group(0)
        if len(value.split()) > 1:
            return match.group(0)
        if value.upper() in ("[LOGIN]", "[EMAIL]") or (
            match.end(2) < len(match.string) and match.string[match.end(2)] == "]"
        ):
            return match.group(0)
        # –î–æ—Ä–∞–±–æ—Ç–∫–∞ ‚Äî –µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã (–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ), –Ω–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —ç—Ç–æ –ø–æ–¥–ø–∏—Å—å)
        if value in {"–°", "–° —É–≤–∞–∂–µ–Ω–∏–µ–º"} or (value.istitle() and len(value) <= 3):
            return match.group(0)
        return f"{match.group(1)}: [LOGIN]"

    def mask_password(match):
        start = match.start(1)
        if in_ignore_spans(start):
            return match.group(0)
        value = match.group(2).strip()
        s = match.string
        pos = match.start(1)
        left_context = s[pos - 1] if pos > 0 else ""
        right_context = s[match.end(2)] if match.end(2) < len(s) else ""
        if not value or value in ("–∏", "–∏–ª–∏", "/", ",", "–∏/–∏–ª–∏"):
            return match.group(0)
        if left_context in "\"'¬´" or right_context in "\"'¬ª":
            return match.group(0)
        if len(value.split()) > 1:
            return match.group(0)
        if value.upper() == "[PASSWORD]" or (
            match.end(2) < len(match.string) and match.string[match.end(2)] == "]"
        ):
            return match.group(0)
        # –î–æ—Ä–∞–±–æ—Ç–∫–∞ ‚Äî –µ—Å–ª–∏ –ø–æ—Å–ª–µ "–ø–∞—Ä–æ–ª—å" –∏–¥—ë—Ç "–° —É–≤–∞–∂–µ–Ω–∏–µ–º" –∏–ª–∏ "–°", –Ω–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å!
        if value in {"–°", "–° —É–≤–∞–∂–µ–Ω–∏–µ–º"} or (value.istitle() and len(value) <= 3):
            return match.group(0)
        return f"{match.group(1)}: [PASSWORD]"

    text = re.sub(LOGIN_REGEX, mask_login, text, flags=re.IGNORECASE)
    text = re.sub(PASSWORD_REGEX, mask_password, text, flags=re.IGNORECASE)
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()


def remove_signature_from_message(row):
    message = row["reply_message"]
    staffid = row["staffid"]
    if staffid != 0:
        for sig in staff_signatures.get(staffid, []):
            if sig and sig.strip() and sig in message:
                message = message.replace(sig, "")
    return message


def build_dialogue(messages, upto_idx):
    if upto_idx == 0:
        return ""
    return "\n".join(f"{m['role']}: {m['message']}" for m in messages[:upto_idx])


def normalize_label(text):
    text = text.strip()
    text = text.replace(". [URL]", " [URL]")
    text = re.sub(r"\s+", " ", text)
    return text


def sentence_normalize(text):
    sentence_end = re.compile(r"([.!?‚Ä¶]+)(\s+|$)")
    parts = []
    last = 0
    for m in sentence_end.finditer(text):
        start, end = m.span()
        sentence = text[last:end].strip()
        if sentence:
            sentence = sentence[0].upper() + sentence[1:]
            parts.append(sentence)
        last = end
    if last < len(text):
        sentence = text[last:].strip()
        if sentence:
            sentence = sentence[0].upper() + sentence[1:]
            if not re.search(r"[.!?‚Ä¶]$", sentence):
                sentence += "."
            parts.append(sentence)
    return " ".join(parts)


if __name__ == "__main__":
    engine = get_engine()

    with engine.connect() as conn:
        conn.execute(text("SET NAMES utf8mb3"))
        replies_df = pd.read_sql(repiles_query(), conn)
        signatures_df = pd.read_sql("SELECT id, signature FROM hesk_users", conn)

    filtered_signatures_df = signatures_df[signatures_df["signature"].astype(bool)]
    filtered_signatures_df.loc[:, "signature"] = filtered_signatures_df[
        "signature"
    ].apply(clean_text)

    staff_signatures = defaultdict(set)
    staff_signatures: dict[int, set[str]] = defaultdict(set)

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ
    for sid, sig in zip(
        filtered_signatures_df["id"], filtered_signatures_df["signature"]
    ):
        if sig:
            staff_signatures[sid].add(sig)

    # –†—É—á–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏
    manual_signatures = {
        21: "–° —É–≤–∞–∂–µ–Ω–∏–µ–º, –Ω–∞—á–∞–ª—å–Ω–∏–∫ –æ—Ç–¥–µ–ª–∞ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ü–æ–ø–∫–æ–≤ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –Æ—Ä—å–µ–≤–∏—á",
        26: "–Æ–ª–∏—è –ú–∏—Ç—É–∑–∏–Ω–∞ –û—Ç–¥–µ–ª —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –¶–µ–Ω—Ç—Ä–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –¢–µ–ª.: (3822) 900-157, –≤–Ω—É—Ç—Ä. 1130",
        33: "–ù–∞—á–∞–ª—å–Ω–∏–∫ –¶–ò–¢–° –¢–£–°–£–†. –≥. –¢–æ–º—Å–∫, —É–ª. –ö—Ä–∞—Å–Ω–æ–∞—Ä–º–µ–π—Å–∫–∞—è, 146, –∫–∞–±. 804 –¢–µ–ª. (3822) 701-515 (–≤–Ω—É—Ç—Ä. 2436)",
    }
    for sid, sig in manual_signatures.items():
        staff_signatures[sid].add(sig)

    replies_df = replies_df.fillna("")

    replies_df.loc[:, "subject"] = replies_df["subject"].apply(clean_text)
    replies_df.loc[:, "ticket_message"] = replies_df["ticket_message"].apply(clean_text)
    replies_df.loc[:, "reply_message"] = replies_df["reply_message"].apply(clean_text)
    replies_df.loc[:, "reply_message"] = replies_df.apply(
        remove_signature_from_message, axis=1
    )

    ticket_dialogs = defaultdict(list)
    ticket_messages = {}
    ticket_categories = {}
    ticket_subjects = {}

    for _, row in replies_df.iterrows():
        role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if int(row.staffid) == 0 else "–û–ø–µ—Ä–∞—Ç–æ—Ä"
        message = clean_text(row.reply_message)
        if message:
            ticket_dialogs[row.ticket_id].append({"role": role, "message": message})
        if row.ticket_id not in ticket_messages:
            ticket_messages[row.ticket_id] = clean_text(row.ticket_message)
        if row.ticket_id not in ticket_categories:
            ticket_categories[row.ticket_id] = row.category_title.strip()
        if row.ticket_id not in ticket_subjects:
            ticket_subjects[row.ticket_id] = row.subject

    unwanted_labels = {
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! [URL] [URL].",
        "[URL].",
        "[URL] [URL].",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ò—Å–ø—Ä–∞–≤–∏–ª–∏.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ü–æ–ø—Ä–∞–≤–∏–ª.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ó–∞–º–µ–Ω–∏–ª.",
        "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –û—à–∏–±–∫—É –∏—Å–ø—Ä–∞–≤–∏–ª–∏.",
        "–ü–æ–ø—Ä–∞–≤–∏–ª.",
        "–ì–æ—Ç–æ–≤–æ!",
        "–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! [URL].",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ì–æ—Ç–æ–≤–æ [URL].",
        "–ì–æ—Ç–æ–≤–æ [URL].",
        "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ì–æ—Ç–æ–≤–æ.",
        "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! [URL].",
        "–ì–æ—Ç–æ–≤–æ.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! [URL].",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ì–æ—Ç–æ–≤–æ.",
        "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ì–æ—Ç–æ–≤–æ: [URL].",
        "–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! –ì–æ—Ç–æ–≤–æ.",
        "–ì–æ—Ç–æ–≤–æ).",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –î–æ–±–∞–≤–∏–ª–∏.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –£–±—Ä–∞–ª.",
        "–î–æ–±–∞–≤–∏–ª.",
        "–ó–∞–º–µ–Ω–∏–ª.",
        "–ì–æ—Ç–æ–≤–æ: [URL].",
        "–ü—Ä–æ–±–ª–µ–º–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –û—à–∏–±–∫—É –∏—Å–ø—Ä–∞–≤–∏–ª–∏.",
        "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ.",
        "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ì–æ—Ç–æ–≤–æ [URL].",
        "–í—Å–µ –≥–æ—Ç–æ–≤–æ.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –≥–æ—Ç–æ–≤–æ [URL].",
        "–î–æ–±—Ä—ã–π –¥–µ–Ω—å, –≥–æ—Ç–æ–≤–æ.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –≥–æ—Ç–æ–≤–æ.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –î–æ–±–∞–≤–∏–ª.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ü–æ–ø—Ä–∞–≤–∏–ª–∏.",
        "–ò—Å–ø—Ä–∞–≤–∏–ª–∏.",
        "–û—à–∏–±–∫—É –∏—Å–ø—Ä–∞–≤–∏–ª–∏.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ó–∞–º–µ–Ω–∏–ª–∏.",
        "–£–¥–∞–ª–∏–ª.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ì–æ—Ç–æ–≤–æ - [URL].",
        "–î–æ–±—Ä—ã–π –¥–µ–Ω—å [URL].",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –†–∞–∑–º–µ—Å—Ç–∏–ª.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ì–æ—Ç–æ–≤ [URL].",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –í—Å–µ –≥–æ—Ç–æ–≤–æ.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ü—Ä–∞–≤–∞ –¥–æ–±–∞–≤–∏–ª–∏.",
        "–†–µ—à–µ–Ω–∞.",
        "–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! –ó–∞–º–µ–Ω–∏–ª.",
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –£–¥–∞–ª–∏–ª.",
    }

    dataset = []
    for ticket_id, messages in ticket_dialogs.items():
        idx = max(i for i, m in enumerate(messages) if m["role"] == "–û–ø–µ—Ä–∞—Ç–æ—Ä")

        last_operator_msg = messages[idx]
        label = normalize_label(last_operator_msg["message"].strip())
        label = sentence_normalize(label)
        if label.strip() in unwanted_labels:
            continue
        dialogue = build_dialogue(messages, idx)
        category = ticket_categories.get(ticket_id, "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
        subject = ticket_subjects.get(ticket_id, "–ë–µ–∑ —Ç–µ–º—ã")
        user_msg = ticket_messages.get(ticket_id, "")

        full_text = (
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n"
            f"–¢–µ–º–∞: {subject}\n"
            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_msg}\n"
            f"{dialogue + chr(10) if dialogue else ''}–û–ø–µ—Ä–∞—Ç–æ—Ä: "
        )

        dataset.append(
            {
                "text": full_text,
                "label": label,
                "category_title": category,
                "ticket_id": ticket_id,
                "subject": subject,
                "ticket_message": user_msg,
            }
        )

    # print("üîÑ –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ RuBERT –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")

    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # unique_labels = list({entry["label"] for entry in dataset})

    # def embed_batch(batch):
    #     return model.encode(batch, batch_size=32, show_progress_bar=False)

    # batch_size = 32
    # embeddings = []

    # for i in tqdm(range(0, len(unique_labels), batch_size), desc="–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"):
    #     batch = unique_labels[i : i + batch_size]
    #     embeddings.extend(embed_batch(batch))

    # threshold = 0.92
    # clustering = AgglomerativeClustering(
    #     n_clusters=None,
    #     distance_threshold=1 - threshold,
    #     metric="cosine",
    #     linkage="average",
    # )
    # labels = clustering.fit_predict(embeddings)

    # label_map = {}
    # for cluster_id in set(labels):
    #     indices = [i for i, lbl in enumerate(labels) if lbl == cluster_id]
    #     canonical = unique_labels[indices[0]]
    #     for i in indices:
    #         label_map[unique_labels[i]] = canonical

    # for entry in dataset:
    #     entry["label"] = label_map.get(entry["label"], entry["label"])

    with open("dialogue_dataset.json", "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    # with open("label_map.json", "w", encoding="utf-8") as f:
    #     json.dump(label_map, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(dataset)} –¥–∏–∞–ª–æ–≥–æ–≤")
    # print("‚úÖ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ö–æ–∂–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ dialogue_dataset.json")

    # –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π label
    label_counter = Counter(entry["label"] for entry in dataset)
    sorted_label_counts = sorted(
        label_counter.items(), key=lambda x: x[1], reverse=True
    )

    print("\nüî¢ –¢–æ–ø –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –æ—Ç–≤–µ—Ç–æ–≤ (label):")
    for label, count in sorted_label_counts[:20]:
        print(f"{count:4} √ó {label}")
