# interactive_classified.py
# üß† –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏ RuT5-base

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model_path = "./rut5base-finetuned"
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path)
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)


# –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
def classify_topic(text):
    t = text.lower()
    if any(x in t for x in ["vpn", "—É–¥–∞–ª–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø", "–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç"]):
        return "–¥–æ—Å—Ç—É–ø"
    elif any(x in t for x in ["–æ—à–∏–±–∫–∞", "–Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "1004"]):
        return "–æ—à–∏–±–∫–∞"
    elif any(
        x in t for x in ["mathcad", "office", "windows", "visual studio", "–ª–∏—Ü–µ–Ω–∑–∏"]
    ):
        return "–ü–û"
    elif any(x in t for x in ["–∫—É—Ä—Å", "—Ñ–¥–æ", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏", "–ø–æ–¥–ø–∏—Å–∞–Ω"]):
        return "–∫—É—Ä—Å—ã"
    elif any(x in t for x in ["–ø–æ—á—Ç–∞", "mail", "email"]):
        return "–ø–æ—á—Ç–∞"
    else:
        return "–¥—Ä—É–≥–æ–µ"


print("\U0001f4ac –í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–ª–∏ 'exit'):")

while True:
    user_input = input("\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: ")
    if user_input.lower() in {"exit", "quit"}:
        break

    topic = classify_topic(user_input)
    prompt = f"–¢–ï–ú–ê: {topic}\n–ó–∞–ø—Ä–æ—Å: {user_input.strip()}"

    input_ids = tokenizer.encode(
        prompt, return_tensors="pt", truncation=True, max_length=256
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(input_ids, max_length=64)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {response}")
