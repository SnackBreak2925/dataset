# rework/interactive.py

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model_path = "./rut5base-finetuned"  # –£–∫–∞–∂–∏ —Å–≤–æ–π –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
model = T5ForConditionalGeneration.from_pretrained(model_path)
tokenizer = T5Tokenizer.from_pretrained(model_path)

model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("üí¨ –í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–ª–∏ 'exit'):")

while True:
    user_input = input("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: ")
    if user_input.lower() in {"exit", "quit"}:
        break

    prompt = f"–ü–µ—Ä–≤–∏—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {user_input.strip()}"
    input_ids = tokenizer.encode(
        prompt, return_tensors="pt", truncation=True, max_length=256
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(input_ids, max_length=64)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {response}\n")
