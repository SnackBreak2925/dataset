from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

from amazing_printer import ap


def postprocess_answer(answer):
    if "–û–ø–µ—Ä–∞—Ç–æ—Ä:" in answer:
        answer = answer.split("–û–ø–µ—Ä–∞—Ç–æ—Ä:")[-1]
    if "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:" in answer:
        answer = answer.split("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:")[0]
    answer = answer.strip().split("\n\n")[0].strip()
    return answer.strip()


model_path = "./rugpt3small-finetuned"
model = AutoModelForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("üí¨ –í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–ª–∏ 'exit'):")

while True:
    prompt = "–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç\n–¢–µ–º–∞: —Ç–µ—Å—Ç\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: kldghlajgnhflksbsm\n–û–ø–µ—Ä–∞—Ç–æ—Ä: "
    encoded = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            attention_mask=attention_mask,  # –í–æ—Ç —ç—Ç–æ –≥–ª–∞–≤–Ω–æ–µ!
            max_new_tokens=128,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    ap(f"promt {prompt}")
    ap("=" * 100)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    ap(f"ü§ñ –ú–æ–¥–µ–ª—å: {postprocess_answer(response)}")
    break
