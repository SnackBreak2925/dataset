# rag_pipeline_by_category.py
# üß† RAG‚Äë–ø–∞–π–ø–ª–∞–π–Ω —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–µ–π, —É–¥–∞–ª–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤

import json
import os
import logging
import csv
import datetime
import re
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sentence_transformers import SentenceTransformer, util

# ============ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—É—Ç–µ–π ==========
logging.basicConfig(
    filename="rag_debug.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)
logger.info("\n=== RAG‚Äë—Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω ===")

MODEL_PATH = "./rut5base-finetuned"
KB_DIR = "kb_by_category"
CSV_LOG = "rag_answers_log.csv"
TOP_K = 5
SOFT_THRESHOLD = 0.1
ANSWER_MIN_LEN = 15
PATTERNS_TO_SKIP = [r"^\[SIGNATURE\]$", r"\[URL\]"]

# ============ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ==========
logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).eval()
tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)
model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

retriever = SentenceTransformer("all-MiniLM-L6-v2")

# ============ –ó–∞–≥—Ä—É–∑–∫–∞ KB —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ==========
logger.info("–ß—Ç–µ–Ω–∏–µ KB –∏–∑ %s", KB_DIR)
category_kbs, category_embeddings = {}, {}
category_names = []
for fn in os.listdir(KB_DIR):
    if fn.endswith(".json"):
        cat = fn[:-5]
        with open(os.path.join(KB_DIR, fn), encoding="utf-8") as f:
            entries = json.load(f)
        unique_entries = sorted(set(e.strip() for e in entries if e.strip()))
        category_kbs[cat] = unique_entries
        category_embeddings[cat] = retriever.encode(
            unique_entries, convert_to_tensor=True
        )
        category_names.append(cat)

category_name_embeddings = retriever.encode(category_names, convert_to_tensor=True)
print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(category_kbs)}\n")


# ============ –•–µ–ª–ø–µ—Ä—ã ==========
def safe_encode(text):
    try:
        return retriever.encode(text, convert_to_tensor=True, show_progress_bar=False)
    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ encode: %s", e)
        return None


def auto_detect_category(question, top_n=3):
    q_emb = safe_encode(question)
    if q_emb is None:
        return None
    scores = util.cos_sim(q_emb, category_name_embeddings)[0]
    top_indices = torch.topk(scores, k=top_n).indices.tolist()
    best_score = scores[top_indices[0]].item()
    if best_score < 0.2:
        return None
    return category_names[top_indices[0]]


def is_uninformative(answer):
    if len(answer) < ANSWER_MIN_LEN:
        return True
    for pattern in PATTERNS_TO_SKIP:
        if re.search(pattern, answer, flags=re.IGNORECASE):
            return True
    return False


def generate_with_contexts(category, question, top_k=5):
    kb = category_kbs[category]
    emb = category_embeddings[category]
    q_emb = safe_encode(question)
    if q_emb is None:
        return ["[–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è]"], []

    try:
        hits = util.semantic_search(q_emb, emb, top_k=top_k)[0]
    except Exception as e:
        logger.exception("semantic_search error: %s", e)
        return ["[–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞]"], []

    if not hits:
        return ["[–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤]"], []

    max_score = hits[0]["score"]
    adaptive_threshold = max(SOFT_THRESHOLD, max_score * 0.15)

    responses, results = [], []
    seen_answers = set()

    for hit in hits:
        if hit["score"] < adaptive_threshold:
            continue
        idx = hit["corpus_id"]
        ctx = kb[idx]
        prompt = f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ctx}\n–í–æ–ø—Ä–æ—Å: {question}"
        logger.info("Prompt used:\n%s\n", prompt)
        try:
            ids = tokenizer.encode(
                prompt, return_tensors="pt", truncation=True, max_length=256
            ).to(model.device)
            with torch.no_grad():
                out = model.generate(
                    ids,
                    max_length=64,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    num_return_sequences=1,
                )
            ans = tokenizer.decode(out[0], skip_special_tokens=True)
        except Exception as e:
            logger.exception("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞: %s", e)
            ans = "[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏]"

        if ans in seen_answers or is_uninformative(ans):
            continue

        responses.append(ans)
        results.append((ctx, ans, hit["score"]))
        seen_answers.add(ans)

    return responses, results


# ============ CSV –ª–æ–≥–≥–µ—Ä ==========
def log_to_csv(category, question, results):
    with open(CSV_LOG, mode="a", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        for ctx, ans, score in results:
            writer.writerow(
                [
                    datetime.datetime.now().isoformat(),
                    category,
                    question,
                    ctx,
                    ans,
                    f"{score:.4f}",
                ]
            )


if __name__ == "__main__":
    # ============ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤ ==========
    print("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ 'exit'):")
    while True:
        question = input("–ó–∞–ø—Ä–æ—Å: ").strip()
        if question.lower() in {"exit", "quit"}:
            print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        if not question:
            print("‚ùó –í–æ–ø—Ä–æ—Å –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º.")
            continue

        category = auto_detect_category(question)
        if not category:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞.")
            continue

        print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: {category}")
        answers, results = generate_with_contexts(category, question, top_k=TOP_K)
        if results:
            log_to_csv(category, question, results)
        if not results:
            print("‚ÑπÔ∏è –û—Ç–≤–µ—Ç—ã –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã, –Ω–æ –≤—Å–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã.")

        if not answers:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å.")
            continue

        print("\nüìå –õ—É—á—à–∏–µ –æ—Ç–≤–µ—Ç—ã:")
        for i, (ctx, ans, score) in enumerate(results, 1):
            print(f"{i}) {ans}\n   üîπ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.4f}\n")
